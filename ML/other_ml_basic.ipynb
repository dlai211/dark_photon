{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e5337f0-f9aa-463d-a85a-f64f58f96db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import uproot, wandb, os, logging, json, random\n",
    "import awkward as ak\n",
    "# import torch\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, accuracy_score, log_loss\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# ML model\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, log_loss, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Set up plot defaults\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = 12.0,8.0  # Roughly 11 cm wde by 8 cm high\n",
    "mpl.rcParams['font.size'] = 14.0 # Use 14 point font\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7659f-4b97-4fee-b828-231459743529",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/jlai/ntups/csv/BDT_input_basic.csv'\n",
    "df = pd.read_csv(path)\n",
    "# df = df[df.dphi_phterm_jetterm >= 1.80]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c3b8d-ec84-427b-b61d-3edbff626f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vars = [\n",
    "    'metsig', 'metsigres', 'met', 'met_noJVT', 'dmet', 'ph_pt', 'ph_eta', 'ph_phi',\n",
    "    'jet_central_eta', 'jet_central_pt1', 'jet_central_pt2', 'dphi_met_phterm', 'dphi_met_ph',\n",
    "    'dphi_met_jetterm', 'dphi_phterm_jetterm', 'dphi_ph_centraljet1', 'metplusph', 'failJVT_jet_pt1',\n",
    "    'softerm', 'jetterm', 'jetterm_sumet', 'dphi_met_central_jet', 'balance', 'dphi_jj', 'BDTScore', 'n_jet_central'\n",
    "]\n",
    "\n",
    "Vars2 = [\n",
    "    'metsig', 'met', 'met_noJVT', 'dmet', 'dphi_met_phterm','dphi_ph_centraljet1',\n",
    "    'dphi_phterm_jetterm', 'jetterm', 'dphi_met_central_jet', 'BDTScore', 'weights', 'label', 'process'\n",
    "]\n",
    "\n",
    "Vars3 = [\n",
    "    'metsigres', 'ph_pt', 'ph_eta', 'dphi_met_jetterm', 'failJVT_jet_pt1', 'n_jet_central', 'dphi_jj'\n",
    "]\n",
    "\n",
    "Vars_drop = ['weights', 'label', 'process', 'met', 'dphi_phterm_jetterm']\n",
    "\n",
    "df_Vars2 = df[Vars2].copy()\n",
    "df_Vars3 = df[Vars3].copy()\n",
    "df_Vars3.replace(-999, np.nan, inplace=True)\n",
    "df_Vars3_inverted = 1 / df_Vars3.replace({0: np.nan})  # Avoid division by zero\n",
    "\n",
    "df_ml_input = pd.concat([df_Vars2, df_Vars3_inverted], axis=1)\n",
    "df_ml_input.replace(-999, np.nan, inplace=True)\n",
    "print(\"Number of event with negative weights :\", np.sum(df_ml_input.weights < 0))\n",
    "df_ml_input[\"weights\"] = df_ml_input[\"weights\"].abs() # some of the weights are negative\n",
    "\n",
    "display(df_ml_input.describe())\n",
    "\n",
    "print(\"Number of nan in each variable: \")\n",
    "print(df_ml_input.isna().sum())\n",
    "\n",
    "# Define X (features) and y (labels)\n",
    "X = df_ml_input.drop(Vars_drop, axis=1)\n",
    "y = df_ml_input['label']\n",
    "weights = df_ml_input['weights']\n",
    "\n",
    "# # Reweight signal so that total signal weight = total background weight\n",
    "# sig_mask = df_ml_input['label'] == 1\n",
    "# bkg_mask = df_ml_input['label'] == 0\n",
    "\n",
    "# sum_sig = df_ml_input.loc[sig_mask, 'weights'].sum()\n",
    "# sum_bkg = df_ml_input.loc[bkg_mask, 'weights'].sum()\n",
    "\n",
    "# scale_factor = sum_bkg / sum_sig if sum_sig > 0 else 1.0\n",
    "# df_ml_input.loc[sig_mask, 'weights'] *= scale_factor\n",
    "\n",
    "random_num = random.randint(1, 100)\n",
    "print(\"random number: \", random_num)\n",
    "\n",
    "X_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\n",
    "    X, y, weights, test_size=0.3, random_state=random_num, stratify=y)\n",
    "\n",
    "models = {\n",
    "    \"BDT\": XGBClassifier(\n",
    "        tree_method='hist',\n",
    "        device='cuda',\n",
    "        eval_metric='auc',\n",
    "        missing=np.nan,\n",
    "        random_state=random_num\n",
    "    ),\n",
    "    \"LightGBM\": LGBMClassifier(\n",
    "        boosting_type='gbdt', random_state=random_num\n",
    "    )\n",
    "}\n",
    "\n",
    "roc_curves = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Initialize wandb run for each model\n",
    "    wandb.init(project=\"Dark_ph_ML_Comparison\", name=model_name, reinit=True)\n",
    "\n",
    "    # If model supports sample_weight, fit with it\n",
    "    try:\n",
    "        model.fit(X_train, y_train, sample_weight=sw_train)\n",
    "    except:\n",
    "        model.fit(X_train, y_train)  # Some models like MLP/LogReg may not support sample_weight\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Predict classes\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    metrics = {\n",
    "        \"Train LogLoss\": log_loss(y_train, y_train_pred_proba, sample_weight=sw_train),\n",
    "        \"Test LogLoss\": log_loss(y_test, y_test_pred_proba, sample_weight=sw_test),\n",
    "        \"Train Accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "        \"Test Accuracy\": accuracy_score(y_test, y_test_pred),\n",
    "        \"Train ROC AUC\": roc_auc_score(y_train, y_train_pred_proba, sample_weight=sw_train),\n",
    "        \"Test ROC AUC\": roc_auc_score(y_test, y_test_pred_proba, sample_weight=sw_test)\n",
    "    }\n",
    "\n",
    "    print(metrics)\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log(metrics)\n",
    "\n",
    "    # Save model scores into df\n",
    "    df_ml_input[f\"score_{model_name.lower()}\"] = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Calculate ROC for final plot\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_pred_proba, sample_weight=sw_test)\n",
    "    roc_curves[model_name] = (fpr, tpr, metrics[\"Test ROC AUC\"])\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# --- Final ROC plot for all models ---\n",
    "plt.figure(figsize=(8, 8))\n",
    "for model_name, (fpr, tpr, auc) in roc_curves.items():\n",
    "    plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Comparison of Different ML Models\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"all_models_roc.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
