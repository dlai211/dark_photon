{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d83f5db-f3ae-4feb-b09f-8fb74ea23dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import uproot, sys, time, math, pickle, os, csv, shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import awkward as ak\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.special import betainc\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# import config functions\n",
    "sys.path.append(\"/home/jlai/jet-faking/config\")\n",
    "from jet_faking_26_config import getWeight, zbi, sample_dict, getVarDict\n",
    "from plot_var import variables, variables_data, ntuple_names, ntuple_names_BDT\n",
    "\n",
    "# Set up plot defaults\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = 14.0,10.0  # Roughly 11 cm wde by 8 cm high  \n",
    "mpl.rcParams['font.size'] = 20.0 # Use 14 point font\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "font_size = {\n",
    "    \"xlabel\": 17,\n",
    "    \"ylabel\": 17,\n",
    "    \"xticks\": 15,\n",
    "    \"yticks\": 15,\n",
    "    \"legend\": 14\n",
    "}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\": font_size[\"xlabel\"],  # X and Y axis labels\n",
    "    \"xtick.labelsize\": font_size[\"xticks\"],  # X ticks\n",
    "    \"ytick.labelsize\": font_size[\"yticks\"],  # Y ticks\n",
    "    \"legend.fontsize\": font_size[\"legend\"]  # Legend\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d09cde-73d1-4e91-af82-7b77bd5c32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [04:45<00:00, 40.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logs written to:\n",
      " - ../cutlogs_jet_faking_26_internal/cutflow.log\n",
      " - ../cutlogs_jet_faking_26_internal/cutflow.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------- CONFIG --------\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# LOG_DIR = f\"./cutlogs_{RUN_TAG}\"\n",
    "LOG_DIR = \"../cutlogs_jet_faking_26_internal\"\n",
    "try:\n",
    "    shutil.rmtree(LOG_DIR)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "os.makedirs(LOG_DIR, exist_ok=False)\n",
    "TXT_LOG = os.path.join(LOG_DIR, \"cutflow.log\")\n",
    "CSV_LOG = os.path.join(LOG_DIR, \"cutflow.csv\")\n",
    "\n",
    "ntuple_names = ['ggHyyd','Zjets','Zgamma','Wgamma','Wjets','gammajet_direct', 'data23']\n",
    "\n",
    "def weight_sum(fb, ntuple_name):\n",
    "    if ntuple_name == 'data23':\n",
    "        return float(np.sum(getWeight(fb, ntuple_name, jet_faking=True)))\n",
    "    else:\n",
    "        return float(np.sum(getWeight(fb, ntuple_name)))\n",
    "\n",
    "# ---- logging helpers ----\n",
    "class CutLogger:\n",
    "    def __init__(self, txt_path, csv_path):\n",
    "        self.txt_path = txt_path\n",
    "        self.csv_path = csv_path\n",
    "        if not os.path.exists(csv_path):\n",
    "            with open(csv_path, \"w\", newline=\"\") as f:\n",
    "                w = csv.writer(f)\n",
    "                w.writerow([\"sample\",\"step_idx\",\"step\",\"events\",\"weighted\",\"elapsed_s\"])\n",
    "        # fresh txt header\n",
    "        with open(txt_path, \"a\") as f:\n",
    "            f.write(f\"\\n==== Cutflow run {RUN_TAG} ====\\n\")\n",
    "\n",
    "    def write(self, sample, step_idx, step, events, weighted, elapsed):\n",
    "        # text\n",
    "        with open(self.txt_path, \"a\") as f:\n",
    "            f.write(f\"[{sample:12s}] {step_idx:02d}  {step:30s}  \"\n",
    "                    f\"events={events:8d}  weighted={weighted:.6g}  dt={elapsed:.3f}s\\n\")\n",
    "        # csv\n",
    "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([sample, step_idx, step, int(events), f\"{weighted:.12g}\", f\"{elapsed:.6f}\"])\n",
    "\n",
    "logger = CutLogger(TXT_LOG, CSV_LOG)\n",
    "\n",
    "def log_step(sample, step_idx, step_label, fb, t0):\n",
    "    nevt = len(fb)\n",
    "    wsum = weight_sum(fb, sample)\n",
    "    logger.write(sample, step_idx, step_label, nevt, wsum, time.time() - t0)\n",
    "\n",
    "def require(mask, name):\n",
    "    \"\"\"Utility to guard awkward masks and give readable errors if shapes mismatch.\"\"\"\n",
    "    if isinstance(mask, (np.ndarray, ak.Array)) and ak.num(mask, axis=0) is not None:\n",
    "        return mask\n",
    "    raise RuntimeError(f\"Mask '{name}' has wrong shape/type: {type(mask)}\")\n",
    "\n",
    "# ---- your loop with logging ----\n",
    "for ntuple_name in tqdm(ntuple_names):\n",
    "    start_time = time.time()\n",
    "    step = 0\n",
    "\n",
    "    if ntuple_name == 'data23':  # data-driven\n",
    "        path = \"/data/fpiazza/ggHyyd/Ntuples/MC23d/withVertexBDT/data23_y_BDT_score.root\"\n",
    "        f = uproot.open(path)['nominal']\n",
    "        fb = f.arrays(variables_data, library=\"ak\")\n",
    "        fb['VertexBDTScore'] = fb['BDTScore']\n",
    "\n",
    "        log_step(ntuple_name, step, \"loaded\", fb, start_time); step += 1\n",
    "\n",
    "        # ensure photon arrays exist for reweighting usage downstream\n",
    "        fb = fb[ak.num(fb['ph_eta']) > 0]\n",
    "        # jet-faking-photon cut (data control)\n",
    "        mask = (ak.firsts(fb['ph_topoetcone40']) - 2450.)/ak.firsts(fb['ph_pt']) > 0.1\n",
    "        fb = fb[require(mask, \"jetfake\")]\n",
    "        log_step(ntuple_name, step, \"jet_faking_photon\", fb, start_time); step += 1\n",
    "\n",
    "        fb = fb[fb['n_ph_baseline'] == 1]\n",
    "        log_step(ntuple_name, step, \"n_ph_baseline==1\", fb, start_time); step += 1\n",
    "\n",
    "    else:  # MC\n",
    "        path = f\"/data/tmathew/ntups/mc23d/{ntuple_name}_y.root\" \n",
    "        path_BDT = f\"/data/fpiazza/ggHyyd/Ntuples/MC23d/withVertexBDT/mc23d_{ntuple_name}_y_BDT_score.root\"\n",
    "        f = uproot.open(path)['nominal']\n",
    "        fb = f.arrays(variables, library=\"ak\")\n",
    "\n",
    "        # add BDT score (same file path, same tree)\n",
    "        f_BDT = uproot.open(path_BDT)['nominal']\n",
    "        fb_BDT = f_BDT.arrays([\"event\", \"BDTScore\"], library=\"ak\")\n",
    "        if np.all(fb[\"event\"] == fb_BDT[\"event\"]):\n",
    "            fb[\"VertexBDTScore\"] = fb_BDT[\"BDTScore\"]\n",
    "        else:\n",
    "            print(f\"[WARN] Event mismatch in {ntuple_name}; BDT not attached\")\n",
    "\n",
    "        log_step(ntuple_name, step, \"loaded\", fb, start_time); step += 1\n",
    "\n",
    "        fb = fb[ak.num(fb['ph_eta']) > 0]\n",
    "        fb = fb[fb['n_ph'] == 1]\n",
    "        log_step(ntuple_name, step, \"n_ph==1\", fb, start_time); step += 1\n",
    "\n",
    "        if ntuple_name in (\"Zjets\",\"Wjets\"):\n",
    "            mask = ak.firsts(fb['ph_truth_type']) == 2   # keep e->gamma only\n",
    "            fb = fb[require(mask, \"ph_truth_type==2\")]\n",
    "            log_step(ntuple_name, step, \"truth e->gamma\", fb, start_time); step += 1\n",
    "\n",
    "        if ntuple_name == \"ggHyyd\":\n",
    "            fb = fb[ak.num(fb['pv_z']) > 0]\n",
    "            log_step(ntuple_name, step, \"pv_z exists\", fb, start_time); step += 1\n",
    "            good_pv = (np.abs(ak.firsts(fb['pv_truth_z']) - ak.firsts(fb['pv_z'])) <= 0.5)\n",
    "            fb = fb[require(good_pv, \"goodPV\")]\n",
    "            log_step(ntuple_name, step, \"goodPV\", fb, start_time); step += 1\n",
    "\n",
    "    # --------- BASIC CUTS (shared) ----------\n",
    "    # NOTE: If 'ggHyyd' is signal without a prompt μ, consider not requiring n_mu==1 for that sample.\n",
    "    fb = fb[fb['n_mu_baseline'] == 0]\n",
    "    log_step(ntuple_name, step, \"n_mu_baseline==0\", fb, start_time); step += 1\n",
    "\n",
    "    fb = fb[fb['n_el_baseline'] == 0]\n",
    "    log_step(ntuple_name, step, \"n_el_baseline==0\", fb, start_time); step += 1\n",
    "\n",
    "    fb = fb[fb['n_tau_baseline'] == 0]\n",
    "    log_step(ntuple_name, step, \"n_tau_baseline==0\", fb, start_time); step += 1\n",
    "\n",
    "    fb = fb[fb['trigger_HLT_g50_tight_xe40_cell_xe70_pfopufit_80mTAC_L1eEM26M'] == 1]\n",
    "    log_step(ntuple_name, step, \"trigger==1\", fb, start_time); step += 1\n",
    "\n",
    "    fb = fb[ak.num(fb['ph_pt']) > 0]\n",
    "    fb = fb[ak.firsts(fb['ph_pt']) > 50_000]\n",
    "    log_step(ntuple_name, step, \"ph_pt>50GeV\", fb, start_time); step += 1\n",
    "\n",
    "    fb = fb[fb['met_tst_et'] > 100_000]\n",
    "    log_step(ntuple_name, step, \"MET>100GeV\", fb, start_time); step += 1\n",
    "\n",
    "    fb = fb[fb['n_jet_central'] <= 3]\n",
    "    log_step(ntuple_name, step, \"n_jet_central<=3\", fb, start_time); step += 1\n",
    "\n",
    "    mt_tmp = np.sqrt(2 * fb['met_tst_et'] * ak.firsts(fb['ph_pt']) *\n",
    "                     (1 - np.cos(fb['met_tst_phi'] - ak.firsts(fb['ph_phi'])))) / 1000.0\n",
    "    mask1 = mt_tmp > 100\n",
    "    mask2 = mt_tmp < 140\n",
    "    fb = fb[mask1 * mask2]\n",
    "    log_step(ntuple_name, step, \"140>mT>100GeV\", fb, start_time); step += 1\n",
    "\n",
    "    fb = fb[fb['VertexBDTScore'] > 0.1]\n",
    "    log_step(ntuple_name, step, \"VertexBDTScore>0.1\", fb, start_time); step += 1\n",
    "    \n",
    "    metsig_tmp = fb['met_tst_sig'] \n",
    "    mask1 = metsig_tmp > 6\n",
    "    fb = fb[mask1]\n",
    "    log_step(ntuple_name, step, \"met_tst_sig>6\", fb, start_time); step += 1\n",
    "    # mask2 = metsig_tmp <= 13\n",
    "    # fb = fb[mask1 * mask2]\n",
    "    \n",
    "    ph_eta_tmp = np.abs(ak.firsts(fb['ph_eta']))\n",
    "    fb = fb[ph_eta_tmp < 1.75]\n",
    "    log_step(ntuple_name, step, \"ph_eta<1.75\", fb, start_time); step += 1\n",
    "\n",
    "    dphi_met_phterm_tmp = np.arccos(np.cos(fb['met_tst_phi'] - fb['met_phterm_phi'])) # added cut 3\n",
    "    fb = fb[dphi_met_phterm_tmp > 1.25]\n",
    "    log_step(ntuple_name, step, \"dphi_met_phterm>1.25\", fb, start_time); step += 1\n",
    "\n",
    "    dmet_tmp = fb['met_tst_noJVT_et'] - fb['met_tst_et']\n",
    "    mask1 = dmet_tmp > -10000\n",
    "    fb = fb[mask1]\n",
    "    log_step(ntuple_name, step, \"dmet>-10GeV\", fb, start_time); step += 1\n",
    "\n",
    "    dphi_jj_tmp = fb['dphi_central_jj']\n",
    "    dphi_jj_tmp = ak.where(dphi_jj_tmp == -10, np.nan, dphi_jj_tmp)\n",
    "    dphi_jj_tmp = np.arccos(np.cos(dphi_jj_tmp))\n",
    "    dphi_jj_tmp = ak.where(np.isnan(dphi_jj_tmp), -999, dphi_jj_tmp)\n",
    "    fb = fb[dphi_jj_tmp < 2.5]\n",
    "    log_step(ntuple_name, step, \"dphi_jj_central<2.5\", fb, start_time); step += 1\n",
    "\n",
    "    dphi_met_jetterm_tmp = np.where(fb['met_jetterm_et'] != 0,   # added cut 5\n",
    "                        np.arccos(np.cos(fb['met_tst_phi'] - fb['met_jetterm_phi'])),\n",
    "                        -999)\n",
    "    fb = fb[dphi_met_jetterm_tmp <= 0.75]\n",
    "    log_step(ntuple_name, step, \"dphi_met_jetterm<0.75\", fb, start_time); step += 1\n",
    "\n",
    "    # tot.append(fb) # save the fb for further study\n",
    "\n",
    "    # ---- sanity check for None ----\n",
    "    n_none = int(ak.sum(ak.is_none(fb['met_tst_et'])))\n",
    "    with open(TXT_LOG, \"a\") as ftxt:\n",
    "        ftxt.write(f\"[{ntuple_name:12s}] None-check met_tst_et: {n_none}\\n\")\n",
    "\n",
    "    # optional: free memory\n",
    "    del fb\n",
    "\n",
    "print(f\"\\nLogs written to:\\n - {TXT_LOG}\\n - {CSV_LOG}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4827b439-c672-4e76-8851-45fba897ea81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified cutflow written to:\n",
      "  - ../cutlogs_jet_faking_26_internal/cutflow_unified.csv\n",
      "  - ../cutlogs_jet_faking_26_internal/cutflow_unified.md\n",
      "                  cut   ggHyyd    Zjets   Zgamma   Wgamma    Wjets gammajet_direct   data23 S/sqrt(B) ZBi (σ_b=30%)\n",
      "                 LOAD 4373.655  3.3e+05 5.92e+04 1.08e+05 7.58e+05        3.28e+07 4.07e+07     0.506        -0.199\n",
      "CUT 1 (preprocessing) 1666.807 1.29e+05 4.76e+04 8.17e+04 5.37e+05        2.78e+07  6.9e+06     0.280        -0.199\n",
      "     n_mu_baseline==0 1666.807 1.29e+05 4.76e+04 8.17e+04 5.37e+05        2.78e+07  6.9e+06     0.280        -0.199\n",
      "     n_el_baseline==0 1666.807 3.81e+04 4.76e+04 8.17e+04 5.37e+05        2.78e+07  6.9e+06     0.280        -0.199\n",
      "    n_tau_baseline==0 1647.269 3.74e+04 4.58e+04 7.55e+04 5.31e+05        2.73e+07 6.78e+06     0.279        -0.199\n",
      "           trigger==1  493.269 1156.530  2.1e+04 1.86e+04 5.22e+04        1.22e+06 1.48e+06     0.296        -0.198\n",
      "          ph_pt>50GeV  492.097 1148.446  2.1e+04 1.86e+04 5.19e+04        1.21e+06 1.46e+06     0.296        -0.198\n",
      "           MET>100GeV  108.536  147.987 1.04e+04 7682.007 9629.902        1.81e+05 1.93e+05     0.171        -0.198\n",
      "     n_jet_central<=3   93.887  129.991 9750.390 6797.769 8820.188        1.73e+05 1.64e+05     0.156        -0.198\n",
      "        140>mT>100GeV   55.482    6.643  220.757  444.193  684.300        2000.082 5202.050     0.600        -0.177\n",
      "   VertexBDTScore>0.1   51.200    3.212  191.386  380.350  587.400         767.974 2547.970     0.765        -0.161\n",
      "        met_tst_sig>6   47.031    0.896  173.583  335.642  440.639          34.355  751.980     1.128        -0.109\n",
      "          ph_eta<1.75   41.284    0.668  136.077  253.516  235.297          21.040  517.050     1.210        -0.081\n",
      " dphi_met_phterm>1.25   39.001    0.649  115.229  232.164  221.977          21.040  395.850     1.241        -0.068\n",
      "          dmet>-10GeV   37.878    0.491  111.621  218.025  209.246          14.240  348.090     1.261        -0.060\n",
      "  dphi_jj_central<2.5   36.224    0.418   99.304  195.908  192.049          11.867  296.390     1.284        -0.049\n",
      "dphi_met_jetterm<0.75   36.092    0.418   98.471  194.735  191.547          11.835  292.410     1.285        -0.048\n"
     ]
    }
   ],
   "source": [
    "process_order = ['ggHyyd','Zjets','Zgamma','Wgamma','Wjets','gammajet_direct','data23']\n",
    "\n",
    "preselection_end = {\n",
    "    'ggHyyd':          'goodPV',               # pv_z exists -> goodPV (take the last one)\n",
    "    'Zjets':           'truth e->gamma',\n",
    "    'Wjets':           'truth e->gamma',\n",
    "    'Zgamma':          'n_ph==1',\n",
    "    'Wgamma':          'n_ph==1',\n",
    "    'gammajet_direct': 'n_ph==1',\n",
    "    'data23':          'n_ph_baseline==1',     # jet_faking_photon -> n_ph_baseline==1 (take the last one)\n",
    "}\n",
    "\n",
    "shared_cuts = [\n",
    "    'n_mu_baseline==0',\n",
    "    'n_el_baseline==0',\n",
    "    'n_tau_baseline==0',\n",
    "    'trigger==1',\n",
    "    'ph_pt>50GeV',\n",
    "    'MET>100GeV',\n",
    "    'n_jet_central<=3',\n",
    "    '140>mT>100GeV',\n",
    "    'VertexBDTScore>0.1',\n",
    "    'met_tst_sig>6',\n",
    "    'ph_eta<1.75',\n",
    "    'dphi_met_phterm>1.25',\n",
    "    'dmet>-10GeV',\n",
    "    'dphi_jj_central<2.5',\n",
    "    'dphi_met_jetterm<0.75',\n",
    "]\n",
    "\n",
    "# Load and sanitize the log\n",
    "df = pd.read_csv(CSV_LOG)\n",
    "# Ensure numeric\n",
    "df['weighted'] = pd.to_numeric(df['weighted'], errors='coerce')\n",
    "df['step_idx'] = pd.to_numeric(df['step_idx'], errors='coerce')\n",
    "\n",
    "# Keep last entry per (sample, step)\n",
    "df = (df.sort_values(['sample','step_idx'])\n",
    "        .drop_duplicates(subset=['sample','step'], keep='last'))\n",
    "\n",
    "# Helper to fetch the weighted yield for a given (sample, step label)\n",
    "def get_yield(sample, step_label):\n",
    "    row = df[(df['sample'] == sample) & (df['step'] == step_label)]\n",
    "    if not row.empty:\n",
    "        return float(row['weighted'].iloc[0])\n",
    "    # Fallback: if a step is unexpectedly missing, try using the latest prior step by index\n",
    "    # (shouldn't happen after preselection, but keeps the pipeline resilient)\n",
    "    sample_rows = df[df['sample'] == sample].sort_values('step_idx')\n",
    "    prior = sample_rows[sample_rows['step_idx'] <= sample_rows['step_idx'].max()]\n",
    "    return float(prior['weighted'].iloc[-1]) if not prior.empty else np.nan\n",
    "\n",
    "# 1) LOAD row = 'loaded'\n",
    "rows = ['LOAD', 'CUT 1 (preprocessing)'] + shared_cuts\n",
    "table = pd.DataFrame(index=rows, columns=process_order, dtype=float)\n",
    "\n",
    "for p in process_order:\n",
    "    table.loc['LOAD', p] = get_yield(p, 'loaded')\n",
    "\n",
    "# 2) CUT 1 row = process-specific preselection_end\n",
    "for p in process_order:\n",
    "    end_step = preselection_end[p]\n",
    "    table.loc['CUT 1 (preprocessing)', p] = get_yield(p, end_step)\n",
    "\n",
    "# 3) Shared cuts: same step name for all processes\n",
    "for cut in shared_cuts:\n",
    "    for p in process_order:\n",
    "        table.loc[cut, p] = get_yield(p, cut)\n",
    "\n",
    "# Compute S/sqrt(B) for each row\n",
    "signal_col = 'ggHyyd'\n",
    "data_cols = ['data23']\n",
    "bkg_cols = [c for c in process_order if c not in ([signal_col])]\n",
    "\n",
    "S = table[signal_col].fillna(0.0)\n",
    "B = table[bkg_cols].sum(axis=1).astype(float)\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    ssb = S / np.sqrt(B)\n",
    "    ssb.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "SIGMA_B_FRAC = 0.30  # 30% relative background uncertainty; change as needed\n",
    "# Vectorized ZBi over rows (robust if B<=0 -> 0.0)\n",
    "ZBi = pd.Series(\n",
    "    [zbi(float(s), float(b), sigma_b_frac=SIGMA_B_FRAC) for s, b in zip(S.values, B.values)],\n",
    "    index=table.index, dtype=float\n",
    ")\n",
    "\n",
    "# Pretty output\n",
    "disp = table.copy()\n",
    "def fmt(x):\n",
    "    if pd.isna(x): return 'n/a'\n",
    "    # scientific for very small/large; fixed otherwise\n",
    "    return f\"{x:.3g}\" if (x != 0 and (abs(x) < 1e-2 or abs(x) >= 1e4)) else f\"{x:.3f}\"\n",
    "\n",
    "disp_out = disp.applymap(fmt)\n",
    "disp_out['S/sqrt(B)'] = ssb.map(lambda v: 'n/a' if pd.isna(v) else f\"{v:.3f}\")\n",
    "disp_out[f'ZBi (σ_b={SIGMA_B_FRAC:.0%})'] = ZBi.map(lambda v: 'n/a' if pd.isna(v) else f\"{v:.3f}\")\n",
    "disp_out.insert(0, 'cut', disp_out.index)\n",
    "\n",
    "# Save\n",
    "out_csv = Path(LOG_DIR) / \"cutflow_unified.csv\"\n",
    "out_md  = Path(LOG_DIR) / \"cutflow_unified.md\"\n",
    "disp_out.to_csv(out_csv, index=False)\n",
    "with open(out_md, \"w\") as f:\n",
    "    f.write(disp_out.to_markdown(index=False))\n",
    "\n",
    "print(\"Unified cutflow written to:\")\n",
    "print(f\"  - {out_csv}\")\n",
    "print(f\"  - {out_md}\")\n",
    "print(disp_out[['cut'] + process_order + ['S/sqrt(B)'] + [f'ZBi (σ_b={SIGMA_B_FRAC:.0%})']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa8f76-2754-4810-919b-fb04dbe1c887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
