{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2410a9e-c5cd-440b-8b60-e720ff4184f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import uproot, sys, time, math, pickle, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import awkward as ak\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.special import betainc\n",
    "from scipy.stats import norm\n",
    "\n",
    "# import config functions\n",
    "from jet_faking_plot_config import getWeight, zbi, sample_dict, getVarDict\n",
    "from plot_var import variables, variables_data, ntuple_names, ntuple_names_BDT\n",
    "\n",
    "# Set up plot defaults\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = 14.0,10.0  # Roughly 11 cm wde by 8 cm high  \n",
    "mpl.rcParams['font.size'] = 20.0 # Use 14 point font\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "font_size = {\n",
    "    \"xlabel\": 17,\n",
    "    \"ylabel\": 17,\n",
    "    \"xticks\": 15,\n",
    "    \"yticks\": 15,\n",
    "    \"legend\": 14\n",
    "}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"axes.labelsize\": font_size[\"xlabel\"],  # X and Y axis labels\n",
    "    \"xtick.labelsize\": font_size[\"xticks\"],  # X ticks\n",
    "    \"ytick.labelsize\": font_size[\"yticks\"],  # Y ticks\n",
    "    \"legend.fontsize\": font_size[\"legend\"]  # Legend\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ef3916f-bc11-44f2-9f8d-f3e87ee756ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file:  /data/tmathew/ntups/mc23d/ggHyyd_uy.root\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/tmathew/ntups/mc23d/ggHyyd_uy.root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:43\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/uproot/reading.py:142\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, object_cache, array_cache, custom_classes, decompression_executor, interpretation_executor, **options)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file_path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseek\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m ):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a string, pathlib.Path, an object with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseek\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m methods, or a length-1 dict of \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mfile_path: object_path}, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m     )\n\u001b[0;32m--> 142\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mReadOnlyFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43marray_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marray_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecompression_executor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompression_executor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpretation_executor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpretation_executor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m object_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mroot_directory\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/uproot/reading.py:561\u001b[0m, in \u001b[0;36mReadOnlyFile.__init__\u001b[0;34m(self, file_path, object_cache, array_cache, custom_classes, decompression_executor, interpretation_executor, **options)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_before_create_source()\n\u001b[1;32m    558\u001b[0m source_cls, file_path \u001b[38;5;241m=\u001b[39m uproot\u001b[38;5;241m.\u001b[39m_util\u001b[38;5;241m.\u001b[39mfile_path_to_source_class(\n\u001b[1;32m    559\u001b[0m     file_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\n\u001b[1;32m    560\u001b[0m )\n\u001b[0;32m--> 561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_source \u001b[38;5;241m=\u001b[39m \u001b[43msource_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_before_get_chunks()\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin_chunk_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m _file_header_fields_big\u001b[38;5;241m.\u001b[39msize:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/uproot/source/fsspec.py:45\u001b[0m, in \u001b[0;36mFSSpecSource.__init__\u001b[0;34m(self, file_path, coalesce_config, **options)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_impl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs\u001b[38;5;241m.\u001b[39masync_impl\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/uproot/source/fsspec.py:59\u001b[0m, in \u001b[0;36mFSSpecSource._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor \u001b[38;5;241m=\u001b[39m FSSpecLoopExecutor()\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fsspec/spec.py:1310\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1310\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fsspec/implementations/local.py:201\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fsspec/implementations/local.py:365\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fsspec/implementations/local.py:370\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m--> 370\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[1;32m    372\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/tmathew/ntups/mc23d/ggHyyd_uy.root'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tot = []\n",
    "data = pd.DataFrame()\n",
    "unweighted_bcut, weighted_bcut, unweighted_acut, weighted_acut = [], [], [], []\n",
    "ntuple_names = ['ggHyyd','Zjets','Zgamma','Wgamma','Wjets','gammajet_direct', 'data23']\n",
    "\n",
    "def test(fb):\n",
    "    # checking if there are any none values\n",
    "    mask = ak.is_none(fb['met_tst_et'])\n",
    "    n_none = ak.sum(mask)\n",
    "    print(\"Number of none values: \", n_none)\n",
    "    # if n_none > 0:\n",
    "    #     fb = fb[~mask]\n",
    "    # print(\"Events after removing none values: \", len(fb), ak.sum(ak.is_none(fb['met_tst_et'])))\n",
    "\n",
    "def print_cut(ntuple_name, fb, label):\n",
    "    print(f\"Unweighted Events {label}: \", len(fb))\n",
    "    if ntuple_name == 'data23':\n",
    "        print(f\"Weighted Events {label}: \", sum(getWeight(fb, ntuple_name, jet_faking=True)))\n",
    "    else: \n",
    "        print(f\"Weighted Events {label}: \", sum(getWeight(fb, ntuple_name)))\n",
    "\n",
    "for i in range(len(ntuple_names)):\n",
    "    ucut, wcut = [], []\n",
    "    start_time = time.time()\n",
    "    ntuple_name = ntuple_names[i]\n",
    "    if ntuple_name == 'data23': # data\n",
    "        path = f\"/data/fpiazza/ggHyyd/Ntuples/MC23d/withVertexBDT/data23_y_BDT_score.root\" \n",
    "        print('processing file: ', path)\n",
    "        f = uproot.open(path)['nominal']\n",
    "        fb = f.arrays(variables_data, library=\"ak\")\n",
    "        fb['VertexBDTScore'] = fb['BDTScore'] # renaming BDTScore to ensure this is recognized as Vertex BDT Score\n",
    "        \n",
    "        fb = fb[ak.num(fb['ph_eta']) > 0]     # for abs(ak.firsts(fb['ph_eta'])) to have value to the reweighting\n",
    "                \n",
    "        mask1 = (ak.firsts(fb['ph_topoetcone40'])-2450.)/ak.firsts(fb['ph_pt']) > 0.1   # jet_faking_photon cut\n",
    "        fb = fb[mask1]\n",
    "        fb = fb[fb['n_ph_baseline'] == 1]\n",
    "\n",
    "    else: # MC\n",
    "        path = f\"/data/tmathew/ntups/mc23d/{ntuple_name}_y.root\" \n",
    "        path_BDT = f\"/data/fpiazza/ggHyyd/Ntuples/MC23d/withVertexBDT/mc23d_{ntuple_name}_y_BDT_score.root\" \n",
    "        print('processing file: ', path)\n",
    "        f = uproot.open(path)['nominal']\n",
    "        fb = f.arrays(variables, library=\"ak\")\n",
    "\n",
    "        # add BDT score to fb\n",
    "        f_BDT = uproot.open(path_BDT)['nominal']\n",
    "        fb_BDT = f_BDT.arrays([\"event\", \"BDTScore\"], library=\"ak\")\n",
    "        tmp = fb[\"event\"] == fb_BDT[\"event\"]\n",
    "        if np.all(tmp) == True:\n",
    "            fb[\"VertexBDTScore\"] = fb_BDT[\"BDTScore\"]\n",
    "        else: \n",
    "            print(\"Something is wrong, need arranging\")\n",
    "\n",
    "        fb = fb[ak.num(fb['ph_eta']) > 0]     # for abs(ak.firsts(fb['ph_eta'])) to have value to the reweighting\n",
    "        fb = fb[fb['n_ph'] == 1]\n",
    "        \n",
    "        # Zjets and Wjets (rule out everything except for e->gamma)\n",
    "        if ntuple_name == 'Zjets' or ntuple_name == 'Wjets':\n",
    "            mask = ak.firsts(fb['ph_truth_type']) == 2\n",
    "            fb = fb[mask]\n",
    "        \n",
    "        # goodPV on signal only\n",
    "        if ntuple_name == 'ggHyyd':\n",
    "            fb = fb[ak.num(fb['pv_z']) > 0]\n",
    "            good_pv_tmp = (np.abs(ak.firsts(fb['pv_truth_z']) - ak.firsts(fb['pv_z'])) <= 0.5)\n",
    "            fb = fb[good_pv_tmp]\n",
    "\n",
    "    print_cut(ntuple_name, fb, 'before cut')\n",
    "    wcut.append(sum(getWeight(fb, ntuple_name)))\n",
    "\n",
    "    # fb = fb[fb['n_mu_baseline'] == 0]\n",
    "    fb = fb[fb['n_mu'] == 1]\n",
    "    # wcut.append(sum(getWeight(fb, ntuple_name)))\n",
    "    # fb = fb[fb['n_el_baseline'] == 0]\n",
    "    # wcut.append(sum(getWeight(fb, ntuple_name)))\n",
    "    # fb = fb[fb['n_tau_baseline'] == 0]\n",
    "    # wcut.append(sum(getWeight(fb, ntuple_name)))\n",
    "    # fb = fb[fb['trigger_HLT_g50_tight_xe40_cell_xe70_pfopufit_80mTAC_L1eEM26M']==1]\n",
    "    # wcut.append(sum(getWeight(fb, ntuple_name)))\n",
    "    # fb = fb[ak.num(fb['ph_pt']) > 0] # prevent none values in Tbranch\n",
    "    # fb = fb[ak.firsts(fb['ph_pt']) >= 50000] # ph_pt cut (basic cut)\n",
    "    # wcut.append(sum(getWeight(fb, ntuple_name)))\n",
    "    # fb = fb[fb['met_tst_et'] >= 100000] # MET cut (basic cut)\n",
    "    # wcut.append(sum(getWeight(fb, ntuple_name)))\n",
    "    # fb = fb[fb['n_jet_central'] <= 4] # n_jet_central cut (basic cut)\n",
    "    # wcut.append(sum(getWeight(fb, ntuple_name)))\n",
    "\n",
    "    # mt_tmp = np.sqrt(2 * fb['met_tst_et'] * ak.firsts(fb['ph_pt']) * \n",
    "    #                         (1 - np.cos(fb['met_tst_phi'] - ak.firsts(fb['ph_phi'])))) / 1000\n",
    "    # mask1 = mt_tmp >= 100 # trigger cut\n",
    "    # fb = fb[mask1]\n",
    "    # wcut.append(sum(getWeight(fb, ntuple_name)))\n",
    "\n",
    "    print_cut(ntuple_name, fb, 'after basic cut')\n",
    "\n",
    "\n",
    "    ucut.append(len(fb))\n",
    "\n",
    "    unweighted_acut.append(ucut)\n",
    "    weighted_acut.append(wcut)\n",
    "    test(fb) # check for none value\n",
    "\n",
    "    print(f\"Reading Time for {ntuple_name}: {(time.time()-start_time)} seconds\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    tot.append(fb)\n",
    "\n",
    "    fb = 0\n",
    "    fb_BDT = 0\n",
    "    tmp = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7b67c9-5b2e-471a-9772-acbf02897d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data after basic cut to a csv file for BDT input\n",
    "Vars = [\n",
    "    'balance', \n",
    "    'VertexBDTScore',\n",
    "    'dmet',\n",
    "    'dphi_jj',\n",
    "    'dphi_met_central_jet',\n",
    "    'dphi_met_phterm',\n",
    "    'dphi_met_ph',\n",
    "    'dphi_met_jetterm',\n",
    "    'dphi_phterm_jetterm',\n",
    "    'dphi_ph_centraljet1',\n",
    "    'ph_pt',\n",
    "    'ph_eta',\n",
    "    'ph_phi',\n",
    "    'jet_central_eta',\n",
    "    'jet_central_pt1',\n",
    "    'jet_central_pt2',\n",
    "    'jetterm',\n",
    "    'jetterm_sumet',\n",
    "    'metsig',\n",
    "    'metsigres',\n",
    "    'met',\n",
    "    'met_noJVT',\n",
    "    'metplusph',\n",
    "    'failJVT_jet_pt1',\n",
    "    'softerm',\n",
    "    'n_jet_central',\n",
    "    'mt'\n",
    "]\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for j in range(len(ntuple_names)):\n",
    "    process = ntuple_names[j]\n",
    "    fb = tot[j] \n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    for var in Vars:\n",
    "        var_config = getVarDict(fb, process, var_name=var)\n",
    "        data_dict[var] = var_config[var]['var']\n",
    "    \n",
    "    weights = getWeight(fb, process)\n",
    "    data_dict['weights'] = weights\n",
    "    \n",
    "    n_events = len(weights)\n",
    "    data_dict['process'] = [process] * n_events\n",
    "    label = 1 if process == 'ggHyyd' else 0\n",
    "    data_dict['label'] = [label] * n_events\n",
    "    \n",
    "    df_temp = pd.DataFrame(data_dict)\n",
    "    data_list.append(df_temp)\n",
    "\n",
    "df_all = pd.concat(data_list, ignore_index=True)\n",
    "df_all.head()\n",
    "\n",
    "df_all.to_csv(\"/data/jlai/ntups/csv/jet_faking_BDT_input_basic3.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0975f1-0afb-4c13-8f34-3f40dd84fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_name = 'ggHyyd'  # Define signal dataset\n",
    "cut_name = 'basic'\n",
    "\n",
    "def getCutDict():\n",
    "    cut_dict = {}\n",
    "\n",
    "    cut_dict['VertexBDTScore'] = {\n",
    "        'lowercut': np.arange(0, 0.4+0.1, 0.1) # VertexBDTScore > cut\n",
    "    }\n",
    "    cut_dict['balance'] = {\n",
    "        'lowercut': np.arange(0, 1.5 + 0.05, 0.05), # balance > cut\n",
    "        'uppercut': np.arange(1.5, 6, 0.2) # balance < cut\n",
    "    }\n",
    "    cut_dict['dmet'] = {\n",
    "        'lowercut': np.arange(-30000, 10000 + 5000, 5000), # dmet > cut\n",
    "        'uppercut': np.arange(10000, 100000 + 5000, 5000), # -10000 < dmet < cut\n",
    "    }\n",
    "    cut_dict['dphi_jj'] = {\n",
    "        'uppercut': np.arange(1, 3.1 + 0.1, 0.1) # dphi_jj < cut\n",
    "    }\n",
    "    cut_dict['dphi_met_jetterm'] = {\n",
    "        'lowercut': np.arange(0, 1 + 0.05, 0.05), # dphi_met_jetterm > cut \n",
    "        'uppercut': np.arange(0.5, 2 + 0.05, 0.05), # dphi_met_jetterm < cut \n",
    "    }\n",
    "    cut_dict['dphi_met_phterm'] = {\n",
    "        'lowercut': np.arange(1, 2 + 0.05, 0.05), # dphi_met_phterm > cut\n",
    "        'uppercut': np.arange(2, 3.1 + 0.1, 0.1), # dphi_met_phterm < cut\n",
    "    }\n",
    "    cut_dict['dphi_ph_centraljet1'] = {\n",
    "        'lowercut': np.arange(0, 2.5 + 0.1, 0.1), # dphi_ph_centraljet1 > cut\n",
    "        'uppercut': np.arange(1.5, 3.1 + 0.1, 0.1) # dphi_ph_centraljet1 < cut\n",
    "    }\n",
    "    cut_dict['dphi_phterm_jetterm'] = {\n",
    "        'lowercut': np.arange(1, 2.5 + 0.05, 0.05), # dphi_phterm_jetterm > cut\n",
    "        'uppercut': np.arange(2, 4 + 0.1, 0.1) # dphi_phterm_jetterm < cut\n",
    "    }\n",
    "    cut_dict['met'] = {\n",
    "        'lowercut': np.arange(100000, 140000 + 5000, 5000),  # met > cut\n",
    "        'uppercut': np.arange(140000, 300000 + 5000, 5000),  # met < cut\n",
    "    }\n",
    "    cut_dict['metsig'] = {\n",
    "        'lowercut': np.arange(0, 10 + 1, 1), # metsig > cut\n",
    "        'uppercut': np.arange(10, 30 + 1, 1), # metsig < cut \n",
    "    }\n",
    "    # cut_dict['mt'] = {\n",
    "    #     'lowercut': np.arange(80, 130+5, 5), # mt > cut\n",
    "    #     'uppercut': np.arange(120, 230+5, 5) # mt < cut\n",
    "    # }\n",
    "    cut_dict['n_jet_central'] = {\n",
    "        'uppercut': np.arange(0, 8+1, 1) # njet < cut\n",
    "    }\n",
    "    cut_dict['ph_eta'] = {\n",
    "        'uppercut': np.arange(1, 2.5 + 0.05, 0.05), # ph_eta < cut\n",
    "    }\n",
    "    cut_dict['ph_pt'] = {\n",
    "        'lowercut': np.arange(50000, 100000 + 5000, 5000),  # ph_pt > cut\n",
    "        'uppercut': np.arange(100000, 300000 + 10000, 10000),  # ph_pt > cut\n",
    "    }\n",
    "\n",
    "    return cut_dict\n",
    "cut_config = getCutDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f3c07e-39df-4ee7-a1b1-5180c97f2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_name = 'ggHyyd'  # Define signal dataset\n",
    "cut_name = 'basic'\n",
    "\n",
    "def getCutDict():\n",
    "    cut_dict = {}\n",
    "\n",
    "    cut_dict['dphi_jj'] = {\n",
    "        'uppercut': np.arange(1, 3.14 + 0.01, 0.01) # dphi_jj < cut\n",
    "    }\n",
    "    cut_dict['dphi_phterm_jetterm'] = {\n",
    "        'lowercut': np.arange(1, 2.5 + 0.05, 0.05), # dphi_phterm_jetterm > cut\n",
    "        'uppercut': np.arange(2, 4 + 0.1, 0.1) # dphi_phterm_jetterm < cut\n",
    "    }\n",
    "    cut_dict['jet_central_eta'] = {\n",
    "        'lowercut': np.arange(-2.5, 0+0.01, 0.01), # jet_central_eta > cut\n",
    "        'uppercut': np.arange(0, 2.5+0.01, 0.01) # jet_central_eta < cut\n",
    "    }\n",
    "    cut_dict['jet_central_pt2'] = {\n",
    "        'lowercut': np.arange(20000, 100000+1000, 1000) # jet_central_pt2 > cut\n",
    "    }\n",
    "    cut_dict['metsigres'] = {\n",
    "        'lowercut': np.arange(8600, 15000, 100),\n",
    "        'uppercut': np.arange(12000, 60000, 100)\n",
    "        \n",
    "    }\n",
    "    cut_dict['met_noJVT'] = {\n",
    "        'lowercut': np.arange(50000, 120000, 100),\n",
    "        'uppercut': np.arange(100000, 250000, 100)\n",
    "    }\n",
    "    cut_dict['softerm'] = {\n",
    "        'uppercut': np.arange(10000, 40000, 100)\n",
    "    }\n",
    "    cut_dict['n_jet_central'] = {\n",
    "        'uppercut': np.arange(0, 8+1, 1) # njet < cut\n",
    "    }\n",
    "\n",
    "    return cut_dict\n",
    "cut_config = getCutDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc325ec4-b60e-4222-b395-d72e8e156598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cut_var': 'dphi_jj', 'cut_type': 'uppercut', 'best_cut': 2.7600000000000016, 'best_sig_x_acc': 0.3486244955136403, 'significance': 0.3722073510197181, 'acceptance': 93.6640543392093}\n",
      "{'cut_var': 'dphi_phterm_jetterm', 'cut_type': 'lowercut', 'best_cut': 1.4500000000000004, 'best_sig_x_acc': 0.5118064096607197, 'significance': 0.6000596689628758, 'acceptance': 85.29258607653298}\n",
      "{'cut_var': 'dphi_phterm_jetterm', 'cut_type': 'uppercut', 'best_cut': 3.000000000000001, 'best_sig_x_acc': 0.3464933370130756, 'significance': 0.3475026838622216, 'acceptance': 99.70954271836754}\n",
      "jet_central_eta 0 251\n",
      "jet_central_eta 250 251\n",
      "jet_central_pt2 0 81\n",
      "{'cut_var': 'metsigres', 'cut_type': 'lowercut', 'best_cut': 10300, 'best_sig_x_acc': 0.3561096438700701, 'significance': 0.36114757116108814, 'acceptance': 98.60502251896057}\n",
      "{'cut_var': 'metsigres', 'cut_type': 'uppercut', 'best_cut': 34200, 'best_sig_x_acc': 0.352821986471691, 'significance': 0.36368004520505426, 'acceptance': 97.01439249237853}\n",
      "{'cut_var': 'met_noJVT', 'cut_type': 'lowercut', 'best_cut': 100000, 'best_sig_x_acc': 0.39697413113028845, 'significance': 0.4111651505188965, 'acceptance': 96.54858409797164}\n",
      "{'cut_var': 'met_noJVT', 'cut_type': 'uppercut', 'best_cut': 249100, 'best_sig_x_acc': 0.30492096917730693, 'significance': 0.31645089168218166, 'acceptance': 96.35648917164231}\n",
      "softerm 299 300\n",
      "{'cut_var': 'n_jet_central', 'cut_type': 'uppercut', 'best_cut': 4, 'best_sig_x_acc': 0.3274664572853601, 'significance': 0.3274664683032898, 'acceptance': 99.99999663540218}\n"
     ]
    }
   ],
   "source": [
    "def get_best_cut(cut_values, significance_list):\n",
    "    max_idx = np.argmax(significance_list)\n",
    "    best_cut = cut_values[max_idx]\n",
    "    best_sig = significance_list[max_idx]\n",
    "    return best_cut, best_sig, max_idx\n",
    "\n",
    "def calculate_significance(cut_var, cut_type, cut_values, tot2, ntuple_names, signal_name, getVarDict, getWeight):\n",
    "    sig_simple_list = []\n",
    "    sigacc_simple_list = []\n",
    "    acceptance_values = []\n",
    "    tot_tmp = []\n",
    "\n",
    "    for cut in cut_values:\n",
    "        sig_after_cut = 0\n",
    "        bkg_after_cut = []\n",
    "        sig_events = 0\n",
    "\n",
    "        for i in range(len(ntuple_names)):\n",
    "            fb = tot2[i]\n",
    "            process = ntuple_names[i]\n",
    "            var_config = getVarDict(fb, process, var_name=cut_var)\n",
    "            x = var_config[cut_var]['var']\n",
    "            mask = x != -999\n",
    "            x = x[mask]\n",
    "\n",
    "            if process == signal_name:\n",
    "                sig_events = getWeight(fb, process)\n",
    "                sig_events = sig_events[mask]\n",
    "                mask = x >= cut if cut_type == 'lowercut' else x <= cut\n",
    "                sig_after_cut = ak.sum(sig_events[mask])\n",
    "            else:\n",
    "                bkg_events = getWeight(fb, process)\n",
    "                bkg_events = bkg_events[mask]\n",
    "                mask = x >= cut if cut_type == 'lowercut' else x <= cut\n",
    "                bkg_after_cut.append(ak.sum(bkg_events[mask]))\n",
    "\n",
    "            \n",
    "            tot_tmp.append(fb)\n",
    "\n",
    "        total_bkg = sum(bkg_after_cut)\n",
    "        total_signal = sig_after_cut\n",
    "\n",
    "        sig_simple = total_signal / np.sqrt(total_bkg) if total_bkg > 0 else 0\n",
    "        acceptance = total_signal / sum(sig_events) if sum(sig_events) > 0 else 0\n",
    "\n",
    "        sig_simple_list.append(sig_simple)\n",
    "        sigacc_simple_list.append(sig_simple * acceptance)\n",
    "        acceptance_values.append(acceptance * 100)\n",
    "\n",
    "    return sig_simple_list, sigacc_simple_list, acceptance_values\n",
    "\n",
    "# os.makedirs(\"plot_data\", exist_ok=True)\n",
    "initial_cut = []\n",
    "tot2 = tot\n",
    "\n",
    "# < -- Initial Cut on all variables (maximize the significance * acceptance) -- > \n",
    "for cut_var, cut_types in cut_config.items():\n",
    "    for cut_type, cut_values in cut_types.items():\n",
    "        sig_simple_list, sigacc_simple_list, acceptance_values = calculate_significance(\n",
    "            cut_var, cut_type, cut_values, tot2, ntuple_names, signal_name, getVarDict, getWeight\n",
    "        )\n",
    "\n",
    "        best_cut, best_sig, idx = get_best_cut(cut_values, sigacc_simple_list) \n",
    "        \n",
    "        if idx == 0 or idx == len(sigacc_simple_list) - 1: # I chose to use index to indicate not to make unnecessary cut (for initial cut)\n",
    "            print(cut_var, idx, len(sigacc_simple_list))\n",
    "            continue\n",
    "            \n",
    "        result = {\n",
    "            \"cut_var\": cut_var,\n",
    "            \"cut_type\": cut_type,\n",
    "            \"best_cut\": best_cut,\n",
    "            \"best_sig_x_acc\": best_sig,\n",
    "            \"significance\": sig_simple_list[idx],\n",
    "            \"acceptance\": acceptance_values[idx]\n",
    "        }\n",
    "\n",
    "        print(result)\n",
    "        initial_cut.append(dict(list(result.items())[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81fd8ae-ad65-4c67-8af0-d313b3c31989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after initial cutting, signficance:  0.7898281381978879\n"
     ]
    }
   ],
   "source": [
    "def apply_cut_to_fb(fb, process, var, cut_val, cut_type, getVarDict):\n",
    "    var_config = getVarDict(fb, process, var_name=var)\n",
    "    x = var_config[var]['var']\n",
    "    mask = x != -999\n",
    "\n",
    "    if cut_type == 'lowercut':\n",
    "        mask = mask & (x >= cut_val)\n",
    "    elif cut_type == 'uppercut':\n",
    "        mask = mask & (x <= cut_val)\n",
    "\n",
    "    return fb[mask]\n",
    "\n",
    "def apply_all_cuts(tot2, ntuple_names, cut_list, getVarDict):\n",
    "    new_tot2 = []\n",
    "    for i, fb in enumerate(tot2):\n",
    "        process = ntuple_names[i]\n",
    "        for cut in cut_list:\n",
    "            fb = apply_cut_to_fb(fb, process, cut[\"cut_var\"], cut[\"best_cut\"], cut[\"cut_type\"], getVarDict)\n",
    "        new_tot2.append(fb)\n",
    "    return new_tot2\n",
    "    \n",
    "def compute_total_significance(tot2, ntuple_names, signal_name, getVarDict, getWeight):\n",
    "    signal_sum = 0\n",
    "    bkg_sum = 0\n",
    "    for i in range(len(ntuple_names)):\n",
    "        fb = tot2[i]\n",
    "        process = ntuple_names[i]\n",
    "        weights = getWeight(fb, process)\n",
    "        if process == signal_name:\n",
    "            signal_sum += ak.sum(weights)\n",
    "        else:\n",
    "            bkg_sum += ak.sum(weights)\n",
    "    return signal_sum / np.sqrt(bkg_sum) if bkg_sum > 0 else 0\n",
    "\n",
    "tot2_initial_cut = apply_all_cuts(tot2, ntuple_names, initial_cut, getVarDict)\n",
    "final_significance = compute_total_significance(tot2_initial_cut, ntuple_names, signal_name, getVarDict, getWeight)\n",
    "print('after initial cutting, signficance: ', final_significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a1941a-3590-4801-ad67-a9fc155ba483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iteration 1 ---\n",
      "Updating dphi_jj (uppercut): 2.7600000000000016 → 1.5500000000000005  (sig 0.79 → 0.82)\n",
      "Updating dphi_phterm_jetterm (lowercut): 1.4500000000000004 → 1.6500000000000006  (sig 0.82 → 0.86)\n",
      "Updating dphi_phterm_jetterm (uppercut): 3.000000000000001 → 2.8000000000000007  (sig 0.86 → 0.89)\n",
      "Updating metsigres (lowercut): 10300 → 10700  (sig 0.89 → 0.89)\n",
      "Updating metsigres (uppercut): 34200 → 20100  (sig 0.89 → 1.09)\n",
      "Updating met_noJVT (lowercut): 100000 → 103000  (sig 1.09 → 1.13)\n",
      "Updating met_noJVT (uppercut): 249100 → 248300  (sig 1.13 → 1.13)\n",
      "\n",
      "--- Iteration 2 ---\n",
      "Updating dphi_jj (uppercut): 1.5500000000000005 → 1.5800000000000005  (sig 1.13 → 1.13)\n",
      "Updating dphi_phterm_jetterm (lowercut): 1.6500000000000006 → 1.8000000000000007  (sig 1.13 → 1.15)\n",
      "Updating metsigres (lowercut): 10700 → 11500  (sig 1.15 → 1.17)\n",
      "Updating metsigres (uppercut): 20100 → 19300  (sig 1.17 → 1.17)\n",
      "Updating met_noJVT (lowercut): 103000 → 103400  (sig 1.17 → 1.18)\n",
      "Updating met_noJVT (uppercut): 248300 → 243100  (sig 1.18 → 1.18)\n",
      "\n",
      "--- Iteration 3 ---\n",
      "Updating dphi_jj (uppercut): 1.5800000000000005 → 1.5600000000000005  (sig 1.18 → 1.18)\n",
      "Updating metsigres (uppercut): 19300 → 19200  (sig 1.18 → 1.18)\n",
      "Updating met_noJVT (lowercut): 103400 → 103000  (sig 1.18 → 1.18)\n",
      "\n",
      "--- Iteration 4 ---\n"
     ]
    }
   ],
   "source": [
    "def n_minus_1_optimizer(initial_cut, cut_config, tot2, ntuple_names, signal_name, getVarDict, getWeight, final_significance, max_iter=10, tolerance=1e-4):\n",
    "    best_cuts = initial_cut.copy()\n",
    "    iteration = 0\n",
    "    converged = False\n",
    "\n",
    "    while not converged and iteration < max_iter:\n",
    "        converged = True\n",
    "        print(f\"\\n--- Iteration {iteration + 1} ---\")\n",
    "        for i, cut in enumerate(best_cuts):\n",
    "            # Apply all other cuts\n",
    "            n_minus_1_cuts = best_cuts[:i] + best_cuts[i+1:]\n",
    "            tot2_cut = apply_all_cuts(tot2, ntuple_names, n_minus_1_cuts, getVarDict)\n",
    "\n",
    "            # Re-scan this variable\n",
    "            cut_var = cut[\"cut_var\"]\n",
    "            cut_type = cut[\"cut_type\"]\n",
    "            cut_values = cut_config[cut_var][cut_type]\n",
    "\n",
    "            sig_simple_list, sigacc_simple_list, _ = calculate_significance(\n",
    "                cut_var, cut_type, cut_values, tot2_cut, ntuple_names\n",
    "                , signal_name, getVarDict, getWeight\n",
    "            )\n",
    "            best_cut, best_sig, idx = get_best_cut(cut_values, sig_simple_list)\n",
    "\n",
    "            if abs(best_cut - cut[\"best_cut\"]) > tolerance:\n",
    "            # if best_sig - final_significance > tolerance:\n",
    "                print(f\"Updating {cut_var} ({cut_type}): {cut['best_cut']} → {best_cut}  (sig {final_significance:.2f} → {best_sig:.2f})\")\n",
    "                best_cuts[i][\"best_cut\"] = best_cut\n",
    "                final_significance = best_sig\n",
    "                converged = False  # Found at least one improvement\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return best_cuts, final_significance\n",
    "\n",
    "# < -- n-1 iterations until no further improvement (max significance) -- >\n",
    "optimized_cuts, final_significance = n_minus_1_optimizer(\n",
    "    initial_cut, cut_config, tot2, ntuple_names, signal_name, getVarDict, getWeight, final_significance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c535d2-87cd-44b2-9ad6-dfecc90ee009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.176777241481177"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cc7e9b6-be01-4bcc-afbf-74a46a8358dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.176777241481177"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot2_optimized_cuts = apply_all_cuts(tot2, ntuple_names, optimized_cuts, getVarDict)\n",
    "compute_total_significance(tot2_optimized_cuts, ntuple_names, signal_name, getVarDict, getWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05d27916-e3cb-4fff-97f8-363b80031d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# < -- Save data after cuts to a csv file for BDT input -- >\n",
    "Vars = [\n",
    "    'balance', \n",
    "    'VertexBDTScore',\n",
    "    'dmet',\n",
    "    'dphi_jj',\n",
    "    'dphi_met_central_jet',\n",
    "    'dphi_met_phterm',\n",
    "    'dphi_met_ph',\n",
    "    'dphi_met_jetterm',\n",
    "    'dphi_phterm_jetterm',\n",
    "    'dphi_ph_centraljet1',\n",
    "    'ph_pt',\n",
    "    'ph_eta',\n",
    "    'ph_phi',\n",
    "    'jet_central_eta',\n",
    "    'jet_central_pt1',\n",
    "    'jet_central_pt2',\n",
    "    'jetterm',\n",
    "    'jetterm_sumet',\n",
    "    'metsig',\n",
    "    'metsigres',\n",
    "    'met',\n",
    "    'met_noJVT',\n",
    "    'metplusph',\n",
    "    'failJVT_jet_pt1',\n",
    "    'softerm',\n",
    "    'n_jet_central'\n",
    "]\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for j in range(len(ntuple_names)):\n",
    "    process = ntuple_names[j]\n",
    "    fb = tot2_optimized_cuts[j] \n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    for var in Vars:\n",
    "        var_config = getVarDict(fb, process, var_name=var)\n",
    "        data_dict[var] = var_config[var]['var']\n",
    "    \n",
    "    weights = getWeight(fb, process)\n",
    "    data_dict['weights'] = weights\n",
    "    \n",
    "    n_events = len(weights)\n",
    "    data_dict['process'] = [process] * n_events\n",
    "    label = 1 if process == 'ggHyyd' else 0\n",
    "    data_dict['label'] = [label] * n_events\n",
    "    \n",
    "    df_temp = pd.DataFrame(data_dict)\n",
    "    data_list.append(df_temp)\n",
    "\n",
    "df_all = pd.concat(data_list, ignore_index=True)\n",
    "df_all.head()\n",
    "\n",
    "df_all.to_csv(\"/data/jlai/ntups/csv/jet_faking_BDT_input_basic_reduced.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006fc72-82aa-4acd-ad6c-1b940d218888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_significance(tot2, ntuple_names, signal_name, getVarDict, getWeight):\n",
    "    signal_sum = 0\n",
    "    bkg_sum = 0\n",
    "    for i in range(len(ntuple_names)):\n",
    "        fb = tot2[i]\n",
    "        process = ntuple_names[i]\n",
    "        weights = getWeight(fb, process)\n",
    "        print(process, ak.sum(weights))\n",
    "        if process == signal_name:\n",
    "            signal_sum += ak.sum(weights)\n",
    "        else:\n",
    "            bkg_sum += ak.sum(weights)\n",
    "    return signal_sum / np.sqrt(bkg_sum) if bkg_sum > 0 else 0\n",
    "\n",
    "tot2_cut = apply_all_cuts(tot2, ntuple_names, cut_tmp, getVarDict)\n",
    "compute_total_significance(tot2_cut, ntuple_names, signal_name, getVarDict, getWeight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e671b72a-fdf0-42bb-8620-52271f84d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_tmp = [{'cut_var': 'VertexBDTScore', 'cut_type': 'lowercut', 'best_cut': 0.10}, \n",
    " {'cut_var': 'dphi_met_phterm', 'cut_type': 'lowercut', 'best_cut': 1.35}, \n",
    " {'cut_var': 'dphi_met_phterm', 'cut_type': 'uppercut', 'best_cut': 3}, \n",
    " {'cut_var': 'metsig', 'cut_type': 'lowercut', 'best_cut': 6},\n",
    " {'cut_var': 'metsig', 'cut_type': 'uppercut', 'best_cut': 16}, \n",
    " {'cut_var': 'ph_eta', 'cut_type': 'uppercut', 'best_cut': 1.75},\n",
    " {'cut_var': 'dmet', 'cut_type': 'lowercut', 'best_cut': -20000},\n",
    " {'cut_var': 'dmet', 'cut_type': 'uppercut', 'best_cut': 50000},\n",
    " {'cut_var': 'dphi_met_jetterm', 'cut_type': 'lowercut', 'best_cut': 0.05},\n",
    " {'cut_var': 'dphi_met_jetterm', 'cut_type': 'uppercut', 'best_cut': 0.65},\n",
    " {'cut_var': 'balance', 'cut_type': 'lowercut', 'best_cut': 0.90},\n",
    " {'cut_var': 'dphi_jj', 'cut_type': 'uppercut', 'best_cut': 2.4}]\n",
    "\n",
    "tot2_cut = apply_all_cuts(tot2, ntuple_names, cut_tmp, getVarDict)\n",
    "sig_simple_list, sigacc_simple_list, acceptance_values = calculate_significance('VertexBDTScore', 'lowercut', np.arange(0, 0.4+0.1, 0.1), tot2_cut, ntuple_names, signal_name, getVarDict, getWeight)\n",
    "best_cut, best_sig, idx = get_best_cut(cut_values, sigacc_simple_list) \n",
    "\n",
    "sig_simple_list[idx], sigacc_simple_list[idx], acceptance_values[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aca6c361-5e2f-455e-bbc5-2eb81790d6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cut_var': 'VertexBDTScore', 'cut_type': 'lowercut', 'best_cut': 0.1},\n",
       " {'cut_var': 'balance', 'cut_type': 'lowercut', 'best_cut': 0.9},\n",
       " {'cut_var': 'dmet', 'cut_type': 'lowercut', 'best_cut': -20000},\n",
       " {'cut_var': 'dmet', 'cut_type': 'uppercut', 'best_cut': 35000},\n",
       " {'cut_var': 'dphi_jj', 'cut_type': 'uppercut', 'best_cut': 3.100000000000002},\n",
       " {'cut_var': 'dphi_met_jetterm', 'cut_type': 'lowercut', 'best_cut': 0.05},\n",
       " {'cut_var': 'dphi_met_jetterm',\n",
       "  'cut_type': 'uppercut',\n",
       "  'best_cut': 0.7000000000000002},\n",
       " {'cut_var': 'dphi_met_phterm',\n",
       "  'cut_type': 'lowercut',\n",
       "  'best_cut': 1.6000000000000005},\n",
       " {'cut_var': 'dphi_met_phterm',\n",
       "  'cut_type': 'uppercut',\n",
       "  'best_cut': 2.900000000000001},\n",
       " {'cut_var': 'dphi_ph_centraljet1',\n",
       "  'cut_type': 'lowercut',\n",
       "  'best_cut': 1.7000000000000002},\n",
       " {'cut_var': 'dphi_phterm_jetterm',\n",
       "  'cut_type': 'lowercut',\n",
       "  'best_cut': 1.9500000000000008},\n",
       " {'cut_var': 'dphi_phterm_jetterm',\n",
       "  'cut_type': 'uppercut',\n",
       "  'best_cut': 3.100000000000001},\n",
       " {'cut_var': 'metsig', 'cut_type': 'lowercut', 'best_cut': 6},\n",
       " {'cut_var': 'metsig', 'cut_type': 'uppercut', 'best_cut': 16},\n",
       " {'cut_var': 'n_jet_central', 'cut_type': 'uppercut', 'best_cut': 4},\n",
       " {'cut_var': 'ph_eta', 'cut_type': 'uppercut', 'best_cut': 1.5500000000000005}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0bf7d1-e31e-4698-99f9-fc0b0634049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_significance(cut_var, cut_type, best_cut, tot_1st_cut, ntuple_names, signal_name, getVarDict, getWeight):\n",
    "    sig_simple_list = []\n",
    "    sigacc_simple_list = []\n",
    "    acceptance_values = []\n",
    "    tot_tmp = []\n",
    "\n",
    "    for cut in cut_values:\n",
    "        sig_after_cut = 0\n",
    "        bkg_after_cut = []\n",
    "        sig_events = 0\n",
    "\n",
    "        for i in range(len(ntuple_names)):\n",
    "            fb = tot2[i]\n",
    "            process = ntuple_names[i]\n",
    "            var_config = getVarDict(fb, process, var_name=cut_var)\n",
    "            x = var_config[cut_var]['var']\n",
    "            mask = x != -999\n",
    "            x = x[mask]\n",
    "\n",
    "            if process == signal_name:\n",
    "                sig_events = getWeight(fb, process)\n",
    "                sig_events = sig_events[mask]\n",
    "                mask = x >= cut if cut_type == 'lowercut' else x <= cut\n",
    "                sig_after_cut = ak.sum(sig_events[mask])\n",
    "            else:\n",
    "                bkg_events = getWeight(fb, process)\n",
    "                bkg_events = bkg_events[mask]\n",
    "                mask = x >= cut if cut_type == 'lowercut' else x <= cut\n",
    "                bkg_after_cut.append(ak.sum(bkg_events[mask]))\n",
    "\n",
    "            \n",
    "            tot_tmp.append(fb)\n",
    "\n",
    "        total_bkg = sum(bkg_after_cut)\n",
    "        total_signal = sig_after_cut\n",
    "\n",
    "        sig_simple = total_signal / np.sqrt(total_bkg) if total_bkg > 0 else 0\n",
    "        acceptance = total_signal / sum(sig_events) if sum(sig_events) > 0 else 0\n",
    "\n",
    "        sig_simple_list.append(sig_simple)\n",
    "        sigacc_simple_list.append(sig_simple * acceptance)\n",
    "        acceptance_values.append(acceptance * 100)\n",
    "\n",
    "    return sig_simple_list, sigacc_simple_list, acceptance_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dffb7c8-faa0-40a8-957e-b5ec2cc290d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    for cut_var, cut_types in cut_config.items():\n",
    "        for cut_type, cut_values in cut_types.items():\n",
    "            sig_simple_list, sigacc_simple_list, acceptance_values = calculate_significance(\n",
    "                cut_var, cut_type, cut_values, tot2, ntuple_names, signal_name, getVarDict, getWeight\n",
    "            )\n",
    "    \n",
    "            best_cut, best_sig, idx = get_best_cut(cut_values, sigacc_simple_list)\n",
    "            result = {\n",
    "                \"cut_var\": cut_var,\n",
    "                \"cut_type\": cut_type,\n",
    "                \"best_cut\": best_cut,\n",
    "                \"best_sig_x_acc\": best_sig,\n",
    "                \"significance\": sig_simple_list[idx],\n",
    "                \"acceptance\": acceptance_values[idx]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc64e1-99b4-4d4b-8eca-5fdb819bae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel(tot):\n",
    "    tot2 = []\n",
    "    for i in range(len(tot)):\n",
    "        fb2 = tot[i]\n",
    "\n",
    "        # jet_sum_tmp = ak.sum(fb2['jet_central_pt'], axis=-1)\n",
    "        # expr = (fb2['met_tst_et'] + ak.firsts(fb2['ph_pt'])) / ak.where(jet_sum_tmp != 0, jet_sum_tmp, 1)\n",
    "        # balance_tmp = ak.where(jet_sum_tmp != 0, expr, -999)\n",
    "        # mask1 = balance_tmp >= 0.65\n",
    "        # mask2 = balance_tmp == -999\n",
    "        # fb2 = fb2[mask1 | mask2]\n",
    "        \n",
    "        # metsig_tmp = fb2['met_tst_sig'] \n",
    "        # mask1 = metsig_tmp >= 7\n",
    "        # mask2 = metsig_tmp <= 13\n",
    "        # fb2 = fb2[mask1 * mask2]\n",
    "        \n",
    "        # ph_eta_tmp = np.abs(ak.firsts(fb2['ph_eta']))\n",
    "        # fb2 = fb2[ph_eta_tmp <= 1.75]\n",
    "\n",
    "        # dphi_met_phterm_tmp = np.arccos(np.cos(fb2['met_tst_phi'] - fb2['met_phterm_phi'])) # added cut 3\n",
    "        # fb2 = fb2[dphi_met_phterm_tmp >= 1.35]\n",
    "\n",
    "        # dmet_tmp = fb2['met_tst_noJVT_et'] - fb2['met_tst_et']\n",
    "        # mask1 = dmet_tmp >= -20000\n",
    "        # mask2 = dmet_tmp <= 50000\n",
    "        # fb2 = fb2[mask1 * mask2]\n",
    "\n",
    "        # phi1_tmp = ak.firsts(fb2['jet_central_phi']) # added cut 7\n",
    "        # phi2_tmp = ak.mask(fb2['jet_central_phi'], ak.num(fb2['jet_central_phi']) >= 2)[:, 1] \n",
    "        # dphi_tmp = np.arccos(np.cos(phi1_tmp - phi2_tmp))\n",
    "        # dphi_jj_tmp = ak.fill_none(dphi_tmp, -999)\n",
    "        # fb2 = fb2[dphi_jj_tmp <= 2.5]\n",
    "\n",
    "        # dphi_met_jetterm_tmp = np.where(fb2['met_jetterm_et'] != 0,   # added cut 5\n",
    "        #                     np.arccos(np.cos(fb2['met_tst_phi'] - fb2['met_jetterm_phi'])),\n",
    "        #                     -999)\n",
    "        # fb2 = fb2[dphi_met_jetterm_tmp <= 0.70]\n",
    "        \n",
    "        tot2.append(fb2)\n",
    "    return tot2\n",
    "\n",
    "# tot2 = sel(tot)\n",
    "tot2 = tot\n",
    "\n",
    "signal_name = 'ggHyyd'  # Define signal dataset\n",
    "cut_name = 'basic'\n",
    "\n",
    "def getCutDict():\n",
    "    cut_dict = {}\n",
    "\n",
    "    cut_dict['dphi_jj'] = {\n",
    "        'lowercut': np.arange(0, 0.4+0.1, 0.1) # BDTScore > cut\n",
    "    }\n",
    "    cut_dict['balance'] = {\n",
    "        'lowercut': np.arange(0, 1.5 + 0.05, 0.05), # balance > cut\n",
    "        'uppercut': np.arange(5, 8 + 0.2, 0.2) # balance < cut\n",
    "    }\n",
    "    cut_dict['dmet'] = {\n",
    "        'lowercut': np.arange(-30000, 0 + 5000, 5000), # dmet > cut\n",
    "        'uppercut': np.arange(10000, 100000 + 5000, 5000), # -10000 < dmet < cut\n",
    "    }\n",
    "    cut_dict['dphi_jj'] = {\n",
    "        'uppercut': np.arange(1, 3.1 + 0.1, 0.1) # dphi_jj < cut\n",
    "    }\n",
    "    cut_dict['dphi_met_jetterm'] = {\n",
    "        'lowercut': np.arange(0, 1 + 0.05, 0.05), # dphi_met_jetterm > cut \n",
    "        'uppercut': np.arange(0.5, 2 + 0.05, 0.05), # dphi_met_jetterm < cut \n",
    "    }\n",
    "    cut_dict['dphi_met_phterm'] = {\n",
    "        'lowercut': np.arange(1, 2 + 0.05, 0.05), # dphi_met_phterm > cut\n",
    "        'uppercut': np.arange(2, 3.1 + 0.1, 0.1), # dphi_met_phterm < cut\n",
    "    }\n",
    "    cut_dict['dphi_ph_centraljet1'] = {\n",
    "        'lowercut': np.arange(0, 2.5 + 0.1, 0.1), # dphi_ph_centraljet1 > cut\n",
    "        'uppercut': np.arange(1.5, 3.1 + 0.1, 0.1) # dphi_ph_centraljet1 < cut\n",
    "    }\n",
    "    cut_dict['dphi_phterm_jetterm'] = {\n",
    "        'lowercut': np.arange(1, 2.5 + 0.05, 0.05), # dphi_phterm_jetterm > cut\n",
    "        'uppercut': np.arange(2, 4 + 0.1, 0.1) # dphi_phterm_jetterm < cut\n",
    "    }\n",
    "    cut_dict['met'] = {\n",
    "        'lowercut': np.arange(100000, 140000 + 5000, 5000),  # met > cut\n",
    "        'uppercut': np.arange(140000, 300000 + 5000, 5000),  # met < cut\n",
    "    }\n",
    "    cut_dict['metsig'] = {\n",
    "        'lowercut': np.arange(0, 10 + 1, 1), # metsig > cut\n",
    "        'uppercut': np.arange(10, 30 + 1, 1), # metsig < cut \n",
    "    }\n",
    "    cut_dict['mt'] = {\n",
    "        'lowercut': np.arange(80, 130+5, 5), # mt > cut\n",
    "        'uppercut': np.arange(120, 230+5, 5) # mt < cut\n",
    "    }\n",
    "    cut_dict['n_jet_central'] = {\n",
    "        'uppercut': np.arange(0, 8+1, 1) # njet < cut\n",
    "    }\n",
    "    cut_dict['ph_eta'] = {\n",
    "        'uppercut': np.arange(1, 2.5 + 0.05, 0.05), # ph_eta < cut\n",
    "    }\n",
    "    cut_dict['ph_pt'] = {\n",
    "        'lowercut': np.arange(50000, 100000 + 5000, 5000),  # ph_pt > cut\n",
    "        'uppercut': np.arange(100000, 300000 + 10000, 10000),  # ph_pt > cut\n",
    "    }\n",
    "\n",
    "    return cut_dict\n",
    "cut_config = getCutDict()\n",
    "\n",
    "def calculate_significance(cut_var, cut_type, cut_values):\n",
    "    sig_simple_list = []\n",
    "    sig_s_plus_b_list = []\n",
    "    sig_s_plus_1p3b_list = []\n",
    "    sig_binomial_list = []\n",
    "\n",
    "    sigacc_simple_list = []\n",
    "    sigacc_s_plus_b_list = []\n",
    "    sigacc_s_plus_1p3b_list = []\n",
    "    sigacc_binomial_list = []\n",
    "\n",
    "    acceptance_values = []  # Store acceptance percentages\n",
    "\n",
    "    for cut in cut_values:\n",
    "        sig_after_cut = 0\n",
    "        bkg_after_cut = []\n",
    "        sig_events = 0\n",
    "        \n",
    "        for i in range(len(ntuple_names)-1): # not include dijet\n",
    "            fb = tot2[i]\n",
    "            process = ntuple_names[i]\n",
    "            var_config = getVarDict(fb, process, var_name=cut_var)\n",
    "            x = var_config[cut_var]['var']\n",
    "            mask = x != -999 # Apply cut: Remove -999 values \n",
    "            x = x[mask]\n",
    "\n",
    "            if process == signal_name:\n",
    "                sig_events = getWeight(fb, process)\n",
    "                sig_events = sig_events[mask]\n",
    "                if cut_type == 'lowercut':\n",
    "                    mask = x >= cut\n",
    "                elif cut_type == 'uppercut':\n",
    "                    mask = x <= cut\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid cut type\")\n",
    "                sig_after_cut = ak.sum(sig_events[mask])\n",
    "            \n",
    "            else:\n",
    "                bkg_events = getWeight(fb, process)\n",
    "                bkg_events = bkg_events[mask]\n",
    "                if cut_type == 'lowercut':\n",
    "                    mask = x >= cut\n",
    "                elif cut_type == 'uppercut':\n",
    "                    mask = x <= cut\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid cut type\")\n",
    "                bkg_after_cut.append(ak.sum(bkg_events[mask]))\n",
    "\n",
    "       # Now compute different types of significance\n",
    "        total_bkg = sum(bkg_after_cut)\n",
    "        total_signal = sig_after_cut\n",
    "\n",
    "        # Avoid zero division carefully\n",
    "        if total_bkg > 0:\n",
    "            sig_simple = total_signal / np.sqrt(total_bkg)\n",
    "            sig_s_plus_b = total_signal / np.sqrt(total_signal + total_bkg) if (total_signal + total_bkg) > 0 else 0\n",
    "            sig_s_plus_1p3b = total_signal / np.sqrt(total_signal + 1.3 * total_bkg) if (total_signal + 1.3*total_bkg) > 0 else 0\n",
    "            sig_binomial = zbi(total_signal, total_bkg, sigma_b_frac=0.3)\n",
    "        else:\n",
    "            sig_simple = sig_s_plus_b = sig_s_plus_1p3b = sig_binomial = 0\n",
    "\n",
    "        # Acceptance\n",
    "        acceptance = total_signal / sum(sig_events) if sum(sig_events) > 0 else 0\n",
    "        acceptance_values.append(acceptance * 100)  # percentage\n",
    "\n",
    "        # Save significance\n",
    "        sig_simple_list.append(sig_simple)\n",
    "        sig_s_plus_b_list.append(sig_s_plus_b)\n",
    "        sig_s_plus_1p3b_list.append(sig_s_plus_1p3b)\n",
    "        sig_binomial_list.append(sig_binomial)\n",
    "\n",
    "        # Save significance × acceptance\n",
    "        sigacc_simple_list.append(sig_simple * acceptance)\n",
    "        sigacc_s_plus_b_list.append(sig_s_plus_b * acceptance)\n",
    "        sigacc_s_plus_1p3b_list.append(sig_s_plus_1p3b * acceptance)\n",
    "        sigacc_binomial_list.append(sig_binomial * acceptance)\n",
    "\n",
    "    return (sig_simple_list, sig_s_plus_b_list, sig_s_plus_1p3b_list, sig_binomial_list,\n",
    "            sigacc_simple_list, sigacc_s_plus_b_list, sigacc_s_plus_1p3b_list, sigacc_binomial_list,\n",
    "            acceptance_values)\n",
    "\n",
    "# Compute significance for each variable dynamically\n",
    "for cut_var, cut_types in cut_config.items():\n",
    "    for cut_type, cut_values in cut_types.items():\n",
    "        (sig_simple_list, sig_s_plus_b_list, sig_s_plus_1p3b_list, sig_binomial_list,\n",
    "         sigacc_simple_list, sigacc_s_plus_b_list, sigacc_s_plus_1p3b_list, sigacc_binomial_list,\n",
    "         acceptance_values) = calculate_significance(cut_var, cut_type, cut_values)\n",
    "\n",
    "        # Plot results\n",
    "        fig, (ax_top, ax_bot) = plt.subplots(2, 1, figsize=(8, 10), sharex=True)\n",
    "\n",
    "        # Top plot: Significance vs. Cut\n",
    "        ax_top.plot(cut_values, sig_simple_list, marker='o', label='S/√B')\n",
    "        # ax_top.plot(cut_values, sig_s_plus_b_list, marker='s', label='S/√(S+B)')\n",
    "        # ax_top.plot(cut_values, sig_s_plus_1p3b_list, marker='^', label='S/√(S+1.3B)')\n",
    "        # ax_top.plot(cut_values, sig_binomial_list, marker='x', label='BinomialExpZ')\n",
    "        ax_top.set_ylabel('Significance')\n",
    "        ax_top.set_title(f'Significance vs. {cut_var} ({cut_type})')\n",
    "        ax_top.legend()\n",
    "        ax_top.grid(True)\n",
    "\n",
    "        # Bottom plot: Significance * Acceptance vs. Cut\n",
    "        ax_bot.plot(cut_values, sigacc_simple_list, marker='o', label='(S/√B) × Acceptance')\n",
    "        # ax_bot.plot(cut_values, sigacc_s_plus_b_list, marker='s', label='(S/√(S+B)) × Acceptance')\n",
    "        # ax_bot.plot(cut_values, sigacc_s_plus_1p3b_list, marker='^', label='(S/√(S+1.3B)) × Acceptance')\n",
    "        # ax_bot.plot(cut_values, sigacc_binomial_list, marker='x', label='BinomialExpZ × Acceptance')\n",
    "\n",
    "        for i, txt in enumerate(acceptance_values):\n",
    "            ax_bot.text(cut_values[i], sigacc_simple_list[i], f'{txt:.1f}%', \n",
    "                        fontsize=10, ha='right', va='bottom', color='purple')\n",
    "            \n",
    "        ax_bot.set_xlabel(f'{cut_var} Cut')\n",
    "        ax_bot.set_ylabel('Significance × Acceptance')\n",
    "        ax_bot.set_title(f'Significance × Acceptance vs. {cut_var} ({cut_type})')\n",
    "        \n",
    "        ax_bot.set_xticks(cut_values)\n",
    "        ax_bot.set_xticklabels(ax_bot.get_xticks(), rotation=45, ha='right')\n",
    "        ax_bot.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "        # ax_bot.xaxis.set_major_locator(ticker.MaxNLocator(nbins=15))  # Show at most 10 x-ticks\n",
    "        \n",
    "        var_configs_tmp = getVarDict(tot2[0], signal_name, cut_var)\n",
    "        ax_bot.set_xlabel(var_configs_tmp[cut_var]['title'])\n",
    "        ax_bot.legend()\n",
    "        ax_bot.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"/data/projects/campfire-workshop/dark_photon/plots/mc23d_{cut_name}cut/significance_{cut_var}_{cut_type}.png\")\n",
    "        print(f\"Successfully saved to /data/projects/campfire-workshop/dark_photon/plots/mc23d_{cut_name}cut/significance_{cut_var}_{cut_type}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967e2635-8d87-4a54-81f5-3f87c1086fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlai/.local/lib/python3.8/site-packages/awkward/_nplikes/array_module.py:251: RuntimeWarning: invalid value encountered in divide\n",
      "  return impl(*broadcasted_args, **(kwargs or {}))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully saved to /data/jlai/dark_photon/jets_faking_photons/lumi135/mc23d_basiccut/dphi_jj_nodijet.png\n",
      "successfully saved to /data/jlai/dark_photon/jets_faking_photons/lumi135/mc23d_basiccut/roc_curve_dphi_jj.png\n"
     ]
    }
   ],
   "source": [
    "def sel(tot):\n",
    "    tot2 = []\n",
    "    for i in range(len(tot)):\n",
    "        fb2 = tot[i]\n",
    "\n",
    "        fb2 = fb2[fb2['BDTScore'] >= 0.1]\n",
    "\n",
    "        dphi_met_phterm_tmp = np.arccos(np.cos(fb2['met_tst_phi'] - fb2['met_phterm_phi']))\n",
    "        fb2 = fb2[dphi_met_phterm_tmp >= 1.35]\n",
    "\n",
    "        metsig_tmp = fb2['met_tst_sig'] \n",
    "        mask1 = metsig_tmp >= 5\n",
    "        mask2 = metsig_tmp <= 16\n",
    "        fb2 = fb2[mask1 * mask2]\n",
    "    \n",
    "        ph_eta_tmp = np.abs(ak.firsts(fb2['ph_eta']))\n",
    "        fb2 = fb2[ph_eta_tmp <= 1.75]\n",
    "\n",
    "        dmet_tmp = fb2['met_tst_noJVT_et'] - fb2['met_tst_et']\n",
    "        mask1 = dmet_tmp >= -20000\n",
    "        mask2 = dmet_tmp <= 50000\n",
    "        fb2 = fb2[mask1 * mask2]\n",
    "\n",
    "        dphi_met_jetterm_tmp = np.where(fb2['met_jetterm_et'] != 0,   # added cut 5\n",
    "                                np.arccos(np.cos(fb2['met_tst_phi'] - fb2['met_jetterm_phi'])),\n",
    "                                -999)\n",
    "        fb2 = fb2[dphi_met_jetterm_tmp <= 0.65]\n",
    "\n",
    "        jet_sum_tmp = ak.sum(fb2['jet_central_pt'], axis=-1)\n",
    "        expr = (fb2['met_tst_et'] + ak.firsts(fb2['ph_pt'])) / ak.where(jet_sum_tmp != 0, jet_sum_tmp, 1)\n",
    "        balance_tmp = ak.where(jet_sum_tmp != 0, expr, -999)\n",
    "        mask1 = balance_tmp >= 0.70\n",
    "        mask2 = balance_tmp == -999\n",
    "        fb2 = fb2[mask1 | mask2]\n",
    "\n",
    "        dphi_jj_tmp = fb2['dphi_central_jj']\n",
    "        dphi_jj_tmp = ak.where(dphi_jj_tmp == -10, np.nan, dphi_jj_tmp)\n",
    "        dphi_jj_tmp = np.arccos(np.cos(dphi_jj_tmp))\n",
    "        dphi_jj_tmp = ak.where(np.isnan(dphi_jj_tmp), -999, dphi_jj_tmp)\n",
    "        fb2 = fb2[dphi_jj_tmp <= 2.4]\n",
    "        \n",
    "        tot2.append(fb2)\n",
    "    return tot2\n",
    "\n",
    "# tot2 = sel(tot)\n",
    "tot2 = tot\n",
    "\n",
    "cut_name = 'basic'\n",
    "var_config = getVarDict(tot2[0], 'ggHyyd', var_name='dphi_jj')\n",
    "# var_config = getVarDict(tot2[0], 'ggHyyd')\n",
    "\n",
    "\n",
    "for var in var_config:\n",
    "    # print(var)\n",
    "    bg_values = []     \n",
    "    bg_weights = []    \n",
    "    bg_colors = []     \n",
    "    bg_labels = []     \n",
    "\n",
    "    signal_values = [] \n",
    "    signal_weights = []\n",
    "    signal_color = None \n",
    "    signal_label = None\n",
    "\n",
    "    for j in range(len(ntuple_names)):\n",
    "    # for j in range(len(ntuple_names)-1): # leave dijet out\n",
    "        process = ntuple_names[j]\n",
    "        fb = tot2[j]  # TTree\n",
    "        var_config = getVarDict(fb, process, var_name=var)\n",
    "\n",
    "        x = var_config[var]['var'] # TBranch\n",
    "        bins = var_config[var]['bins'] \n",
    "\n",
    "        if 'weight' in var_config[var]:  # If weight is there\n",
    "            weights = var_config[var]['weight']\n",
    "        else:\n",
    "            weights = getWeight(fb, process)\n",
    "        \n",
    "        sample_info = sample_dict[process]\n",
    "        color = sample_info['color']\n",
    "        legend = sample_info['legend']\n",
    "\n",
    "        \n",
    "        if process == 'ggHyyd':  # signal\n",
    "            signal_values.append(x)\n",
    "            signal_weights.append(weights)\n",
    "            signal_color = color\n",
    "            signal_label = legend\n",
    "        else:   # background\n",
    "            bg_values.append(x)\n",
    "            bg_weights.append(weights)\n",
    "            bg_colors.append(color)\n",
    "            bg_labels.append(legend)\n",
    "\n",
    "    fig, (ax_top, ax_bot) = plt.subplots(2, 1, figsize=(12, 13), gridspec_kw={'height_ratios': [9, 4]})\n",
    "\n",
    "    ax_top.hist(bg_values, bins=bins, weights=bg_weights, color=bg_colors,\n",
    "                label=bg_labels, stacked=True)\n",
    "\n",
    "    ax_top.hist(signal_values, bins=bins, weights=signal_weights, color=signal_color,\n",
    "                label=signal_label, histtype='step', linewidth=2)\n",
    "\n",
    "    signal_all = np.concatenate(signal_values) if len(signal_values) > 0 else np.array([])\n",
    "    signal_weights_all = np.concatenate(signal_weights) if len(signal_weights) > 0 else np.array([])\n",
    "\n",
    "    # Add error bar for signal (top plot)\n",
    "    if len(signal_all) > 0:\n",
    "        signal_counts, bin_edges = np.histogram(signal_all, bins=bins, weights=signal_weights_all)\n",
    "        sum_weights_sq, _ = np.histogram(signal_all, bins=bins, weights=signal_weights_all**2)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        signal_errors = np.sqrt(sum_weights_sq)  # Poisson error sqrt(N)\n",
    "\n",
    "        ax_top.errorbar(bin_centers, signal_counts, yerr=signal_errors, fmt='.', linewidth=2,\n",
    "                        color=signal_color, capsize=0)\n",
    "\n",
    "    ax_top.set_yscale('log')\n",
    "    ax_top.set_ylim(0.0001, 1e11)\n",
    "    ax_top.set_xlim(bins[0], bins[-1])\n",
    "    ax_top.minorticks_on()\n",
    "    ax_top.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax_top.set_ylabel(\"Events\")\n",
    "    ax_top.legend(ncol=2)\n",
    "    # ax_top.set_title(\"vtx_sumPt distribution\")\n",
    "\n",
    "    bg_all = np.concatenate(bg_values) if len(bg_values) > 0 else np.array([])\n",
    "    bg_weights_all = np.concatenate(bg_weights) if len(bg_weights) > 0 else np.array([])\n",
    "\n",
    "    # Compute the weighted histogram counts using np.histogram\n",
    "    S_counts, _ = np.histogram(signal_all, bins=bins, weights=signal_weights_all)\n",
    "    B_counts, _ = np.histogram(bg_all, bins=bins, weights=bg_weights_all)     \n",
    "\n",
    "    # Compute per-bin significance\n",
    "    sig_simple = np.zeros_like(S_counts, dtype=float)\n",
    "    sig_s_plus_b = np.zeros_like(S_counts, dtype=float)\n",
    "    sig_s_plus_1p3b = np.zeros_like(S_counts, dtype=float)\n",
    "\n",
    "    sqrt_B = np.sqrt(B_counts)\n",
    "    sqrt_SplusB = np.sqrt(S_counts + B_counts)\n",
    "    sqrt_Splus1p3B = np.sqrt(S_counts + 1.3 * B_counts)\n",
    "\n",
    "    # Avoid division by zero safely\n",
    "    sig_simple = np.where(B_counts > 0, S_counts / sqrt_B, 0)\n",
    "    sig_s_plus_b = np.where((S_counts + B_counts) > 0, S_counts / sqrt_SplusB, 0)\n",
    "    sig_s_plus_1p3b = np.where((S_counts + 1.3 * B_counts) > 0, S_counts / sqrt_Splus1p3B, 0)\n",
    "\n",
    "    # Add Binomial ExpZ per bin\n",
    "    zbi_per_bin = np.array([\n",
    "        zbi(S_counts[i], B_counts[i], sigma_b_frac=0.3)\n",
    "        for i in range(len(S_counts))\n",
    "    ])\n",
    "\n",
    "    # Compute the bin centers for plotting\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "    # Compute the total significance: total S / sqrt(total B)\n",
    "    total_signal = np.sum(S_counts)\n",
    "    total_bkg = np.sum(B_counts)\n",
    "\n",
    "    if total_bkg > 0:\n",
    "        total_sig_simple = total_signal / np.sqrt(total_bkg)\n",
    "        total_sig_s_plus_b = total_signal / np.sqrt(total_signal + total_bkg)\n",
    "        total_sig_s_plus_1p3b = total_signal / np.sqrt(total_signal + 1.3 * total_bkg)\n",
    "        total_sig_binomial = zbi(total_signal, total_bkg, sigma_b_frac=0.3)\n",
    "    else:\n",
    "        total_sig_simple = total_sig_s_plus_b = total_sig_s_plus_1p3b = total_sig_binomial = 0\n",
    "\n",
    "    # --- Plot all significance curves ---\n",
    "    ax_bot.step(bin_centers, sig_simple, where='mid', color='chocolate', linewidth=2,\n",
    "                label=f\"S/√B = {total_sig_simple:.4f}\")\n",
    "    ax_bot.step(bin_centers, sig_s_plus_b, where='mid', color='tomato', linewidth=2,\n",
    "                label=f\"S/√(S+B) = {total_sig_s_plus_b:.4f}\")\n",
    "    ax_bot.step(bin_centers, sig_s_plus_1p3b, where='mid', color='orange', linewidth=2,\n",
    "                label=f\"S/√(S+1.3B) = {total_sig_s_plus_1p3b:.4f}\")\n",
    "    ax_bot.step(bin_centers, zbi_per_bin, where='mid', color='plum', linewidth=2,\n",
    "                label=f\"Binomial ExpZ = {total_sig_binomial:.4f}\")\n",
    "\n",
    "    ax_bot.set_xlabel(var_config[var]['title'])\n",
    "    # ax_bot.set_xticks(np.linspace(bins[0], bins[-1], 11))\n",
    "    ax_bot.set_ylabel(\"Significance\")\n",
    "    ax_bot.set_ylim(-0.8, 2)\n",
    "    ax_top.set_xlim(bins[0], bins[-1])\n",
    "\n",
    "    # Do not set a title on the bottom plot.\n",
    "    ax_bot.set_title(\"\")\n",
    "\n",
    "    # Draw a legend with purple text.\n",
    "    leg = ax_bot.legend()\n",
    "    for text in leg.get_texts():\n",
    "        text.set_color('purple')\n",
    "\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"/data/jlai/dark_photon/jets_faking_photons/lumi135/mc23d_{cut_name}cut/{var}_nodijet.png\")\n",
    "    print(f\"successfully saved to /data/jlai/dark_photon/jets_faking_photons/lumi135/mc23d_{cut_name}cut/{var}_nodijet.png\")\n",
    "    plt.close()\n",
    "    # plt.show()\n",
    "\n",
    "    y_true = np.concatenate([np.ones_like(signal_all), np.zeros_like(bg_all)])\n",
    "    # Use the vtx_sumPt values as the classifier output.\n",
    "    y_scores = np.concatenate([signal_all, bg_all])\n",
    "    # Combine the weights for all events.\n",
    "    y_weights = np.concatenate([signal_weights_all, bg_weights_all])\n",
    "\n",
    "    # Compute the weighted ROC curve.\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, sample_weight=y_weights)\n",
    "    sorted_indices = np.argsort(fpr)\n",
    "    fpr_sorted = fpr[sorted_indices]\n",
    "    tpr_sorted = tpr[sorted_indices]\n",
    "\n",
    "    roc_auc = auc(fpr_sorted, tpr_sorted)\n",
    "\n",
    "    # Create a new figure for the ROC curve.\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, lw=2, color='red', label=f'ROC curve (AUC = {roc_auc:.5f})')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random chance')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve for {var}\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.tight_layout()    \n",
    "    plt.savefig(f\"/data/jlai/dark_photon/jets_faking_photons/lumi135/mc23d_{cut_name}cut/roc_curve_{var}.png\")\n",
    "    print(f\"successfully saved to /data/jlai/dark_photon/jets_faking_photons/lumi135/mc23d_{cut_name}cut/roc_curve_{var}.png\")\n",
    "    plt.close()\n",
    "    # plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
